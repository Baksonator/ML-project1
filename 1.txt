a. Word2vec
Word2vec predstavlja jednostavan model masinskog ucenja koji proizvodi “word embedding”, tj. procese u kojima je svaka
rec mapirana u niz brojeva, odnosno vektor. Model je dvoslojna neuralna mreza koja kao ulaz uzima korpus teksta i kao
izlaz daje set vektora gde svaki odgovara jedinstvenoj reci u korpusu. Rezultujuci set vektora zavisi od parametrizacije
modela, od kojih jedan parametar definise dimenziju vektora. Dimenzija vektora predstavlja broj atributa reci koje model
koristi, gde kvalitet modela raste sa povecanjem dimenzije. Ovakvo definisanje reci preko brojeva omogucava primenu
matematickih operacija nad njima. Svrha ovog modela je takodje grupisanje slicnih reci, tj. dobijeni vektori se u
prostoru nalaze u neposrednoj blizini ukoliko reci koje predstavljaju dele zajednicki kontekst u korpusu. Ovo daje
odnose izmedju reci i omogucava siroku primenu word2vec modela u oblastima kao sto je “deep learning”, NLP, analiza
recenica, prepoznavanje govora, bioinformatika itd..

b. K-fold Cross-Validation
K-fold Cross-Validation je statisticki metod koji se koristi za procenu preciznosti naseg modela i menja postupak validacije.

Za razliku od klasicne validacije (podele na trening, validacioni i test skup), gde se preciznost dobijena na jednom validacionom skupu moze
znatno razlikovati u odnosu na preciznost dobijenu na nekom drugom validacionom skupu, uvodimo resenje koje se zasniva na podeli
postojeceg data seta u K manjih grupa (fold-ova) pri cemu se daje mogucnost da se svaka grupa tretira kao grupa validacionih
podataka (validation set) u nekom trenutku, dok se ostatak koristi za treniranje modela (training set). Ovaj postupak se
ponavlja K puta, sto na kraju daje K vrednosti koje prestavljaju razlicite preciznosti u svakoj iteraciji.
Finalna preciznost se dobija prosekom ovih vrednosti.
Obicno se koristi na ogranicenom skupu podataka.

c. Linearna separabilnost
Linearna separabilnost je veoma bitan koncept u statistici, masinskom ucenju i neuralnim mrezama. Linearna separabilnost
je odlika dva skupa tacaka, tj. podataka koje klasifikujemo u dve klase. Osnovna ideja je sledeca: Dva skupa tacaka
, koje predstavljaju n-dimenzionalne vektore, pokusavamo da podelimo koristeci (n - 1)-dimenzionu  hiperravan, tako da
su sve tacke jedne klase sa jedne strane, a sve tacke druge klase sa druge strane te hiperravni. Ukoliko ovo mozemo
uraditi (moze postojati vise mogucih ispravnih hiperravni), onda kazemo da su ta dva skupa (klase) tacaka linearno
separabilna. Ako posmatramo problem u dve dimenzije, dva skupa tacaka su linearno separabilna ukoliko mozemo povuci
liniju izmedju njih, u tri dimenzije ukoliko postoji ravan izmedju njih itd. Ukoliko koristimo najjednostavniju
neuralnu mrezu (perceptron) za klasifikaciju podataka u dve klase, te dve klase moraju biti linearno separabilne kako
bi perceptron davao korektne rezultate. Najpoznatiji primer linearne neseparabilnosti jeste XOR funkcija, gde klase
predstavljaju izlazi (0 ili 1). Proveru linearne separabilnosti dva skupa moze izvrsiti izracunavanjem konveksnog
omotaca jednog i drugog skupa. Ukoliko se konveksni omotaci ne seku, skupovi su linearno separabilni, u suprotnom, nisu.
Vodeci se svim prethodnim cinjenicama, lako mozemo zakljuciti da podaci iz skupa "logreg_data.csv" nisu linearno
separabilni, sto se i jasno moze videti na grafiku koji reprezentuje ove podatke.
